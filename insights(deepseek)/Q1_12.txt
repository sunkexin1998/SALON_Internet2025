To analyze sentiment for software engineering (SE) texts, the study explores the use of large language models (LLMs) in zero-shot and few-shot learning scenarios. Specifically, it evaluates three open-source big LLMs (bLLMs)—Llama 2-Chat, Vicuna, and WizardLM—on five SE datasets (Gerrit, GitHub, Google Play, Jira, and Stack Overflow). The approach involves prompting bLLMs with task descriptions and output formats, and in few-shot settings, providing examples from the training data. The study compares these results with fine-tuned smaller LLMs (sLLMs) like BERT and RoBERTa. The findings suggest that bLLMs perform well in zero-shot settings, especially when labeled data is scarce, while fine-tuned sLLMs excel with ample, balanced training data. The study emphasizes the importance of prompt engineering and dataset characteristics in achieving accurate sentiment analysis for SE texts.