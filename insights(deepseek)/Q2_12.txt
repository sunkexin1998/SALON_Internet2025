Sentiment expression in software engineering (SE) texts differs from general texts due to domain-specific jargon, context, and the collaborative nature of SE. General sentiment analysis tools often perform poorly on SE datasets, necessitating SE-specific tools or models to accurately capture sentiments in reviews, comments, and forum posts.

To handle sentiment analysis in software engineering (SE) texts, the paper suggests using SE-specific sentiment analysis tools or models, as general tools often underperform. Approaches include fine-tuning smaller large language models (sLLMs) like BERT or RoBERTa on SE datasets, or leveraging bigger large language models (bLLMs) like Llama 2-Chat in zero-shot or few-shot settings. bLLMs are particularly effective when labeled data is scarce or imbalanced, while sLLMs excel with ample, balanced training data. Prompt engineering is crucial for bLLMs, and testing multiple prompt templates can optimize performance.