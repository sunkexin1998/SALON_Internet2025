To analyze sentiment in software engineering texts, the paper suggests leveraging both bigger large language models (bLLMs) and smaller large language models (sLLMs). The approach involves using bLLMs in zero-shot and few-shot learning scenarios, where prompts are crafted to guide the model in sentiment classification tasks. The effectiveness of different prompt templates is evaluated, revealing that subtle variations can significantly impact performance. In cases with limited labeled data, bLLMs outperform fine-tuned sLLMs, particularly in imbalanced datasets. Conversely, when ample and balanced training data is available, fine-tuning sLLMs is preferred. The study emphasizes the importance of prompt engineering and the need for consistent labeling practices to enhance sentiment analysis accuracy in software engineering contexts. Additionally, manual error analysis helps identify common misclassification reasons, such as inconsistencies in labeling and subjective interpretations of sentiment.